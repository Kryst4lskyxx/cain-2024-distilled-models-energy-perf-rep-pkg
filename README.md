# The Impact of Knowledge Distillation on the Energy Consumption and Runtime Efficiency of NLP Models
This repository is a companion page for the following publication:
> Ye Yuan, Jingzhi Zhang, Zongyao Zhang, Kaiwei Chen, Jiacheng Shi, Vincenzo Stoico, Ivano Malavolta. The Impact of Knowledge Distillation on the Energy Consumption and Runtime Efficiency of NLP Models. Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering--Software Engineering for AI (CAIN), 2024.

## How to cite us
If this study is helping your research, consider to cite it is as follows, thanks!

```
@inproceedings{cain_2024,
  title={{The Impact of Knowledge Distillation on the Energy Consumption and Runtime Efficiency of NLP Models}},
  author={Ye Yuan and Jingzhi Zhang and Zongyao Zhang and Kaiwei Chen and Jiacheng Shi and Vincenzo Stoico and Ivano Malavolta},
  booktitle={IEEE/ACM 3rd International Conference on AI Engineering--Software Engineering for AI (CAIN)},
  pages={To appear},
  year={2024},
  organization={IEEE}
}
```
